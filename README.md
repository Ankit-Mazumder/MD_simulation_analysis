Spatio-Temporal Graph Transformer for Molecular DynamicsA hybrid Deep Learning framework combining Graph Neural Networks (GINEConv) with Temporal Transformers to predict thermodynamic properties (specifically Gibbs Free Energy, $\Delta G$) from Molecular Dynamics (MD) simulation trajectories.This model captures both the spatial geometry of atoms within a single frame and the temporal evolution of the system across a sliding time window.üß† Model ArchitectureThe model processes sequential windows of molecular graphs (frames) to predict a single scalar value for the window.Data Flow PipelineInput: A window of $T$ frames (default $T=5$). Each frame is a graph $G = (V, E)$.Spatial Encoding (GNN): * 3 layers of GINEConv (Graph Isomorphism Network with Edge features).Extracts atomic interactions and local geometry.Global Pooling: Aggregates node features into a single vector per frame.Feature Fusion: Concatenates Graph Embeddings with Global System Features (Temperature, Pressure, etc.).Temporal Encoding (Transformer):Positional Encoding: Injects time-step order information.Transformer Encoder: 3-Layer Multi-Head Self-Attention mechanism to learn trajectory dynamics.Readout: Regression head predicts $\Delta G$ based on the final state of the sequence.üìÇ Data RequirementsThe script expects three CSV files in the root directory.1. protein_graph_nodes_...csvContains atomic features.Required Columns: frame, atom_index, Z (Atomic Number), mass, rmsd, rmsf, element (categorical), chain (categorical).2. protein_graph_edges_...csvContains bond/interaction features.Required Columns: frame, atom_i, atom_j, bond_type (categorical), distance, angle (degrees).Note: The code automatically converts angle into sin(angle) and cos(angle) components.3. protein_graph_global_...csvContains system-wide variables.Required Columns: frame, plus any global features (e.g., Temperature, PotentialEnergy, GB_4).üõ†Ô∏è InstallationEnsure you have a Python environment (3.8+) with the following dependencies:# Core DL Libraries
pip install torch torchvision
pip install torch-geometric

# Data Handling & Math
pip install pandas numpy scikit-learn

# Plotting
pip install matplotlib seaborn

Note: For torch-geometric, ensure you install the scatter/sparse binaries matching your CUDA version.üöÄ UsageConfigure Paths:Open the script and edit the file paths in the MAIN EXECUTION section:edge_file = 'path/to/edges.csv'
node_file = 'path/to/nodes.csv'
graph_file = 'path/to/global.csv'

Set Target:Define your experimental $\Delta G$ value:EXPERIMENTAL_DELTA_G = -10667.0 

Run the Training:python model_training.py

HyperparametersKey parameters can be adjusted at the top of the MAIN EXECUTION block:WINDOW_SIZE: Number of frames to look back (default: 5).HIDDEN_DIM: Dimension of internal embeddings (default: 128).TRANSFORMER_LAYERS: Depth of the temporal encoder (default: 3).WARMUP_EPOCHS: Epochs to ignore for model saving (default: 20).üìä Outputs & VisualizationUpon completion, the script creates a results_plots/ directory containing:trajectory_dg.png: A time-series plot comparing Predicted vs. Experimental $\Delta G$ across the test trajectory.parity_plot.png: Scatter plot showing correlation ($R^2$ and RMSE) between ground truth and predictions.loss_curve.png: Training and Validation RMSE evolution over epochs.error_distribution.png: Histogram of residual errors.The script also saves the best model weights to best_model_dg.pth.üî¨ Feature Engineering DetailsAngle Transformation: Raw angles (degrees) are automatically converted to cyclic encodings ($\sin(\theta), \cos(\theta)$) to preserve geometric continuity.Scaling: * Node/Edge/Global features are scaled using StandardScaler.The Target $\Delta G$ is scaled by a factor of 1000.0 for numerical stability during backpropagation.Tokenization: Each frame in the window is treated as a "token" in the Transformer sequence, analogous to words in an NLP sentence.üìú LicenseMIT License
